{
 "metadata": {
  "name": "",
  "signature": "sha256:45110c7b61dbe589f7cca8aa07501f3bb88001d5f7efc8058b3366e6d404b2cf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Problem Set 2: Sequence labeling\n",
      "=====================\n",
      "\n",
      "This project focuses on sequence labeling. \n",
      "Part (a) focuses on *generative* approaches, including Naive Bayes and the Hidden Markov Model (HMM).\n",
      "The target domain is Twitter part-of-speech tagging."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scorer, operator\n",
      "from collections import defaultdict, Counter\n",
      "import matplotlib.pyplot as plt\n",
      "%pylab --no-import-all inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 1. Data Processing #\n",
      "\n",
      "Download the train data and dev data from [Github](https://github.com/jacobeisenstein/gt-nlp-class/tree/master/projects/proj-2). \n",
      "The test data will be released around 48 hours before the deadline for Pset 2b.\n",
      "The part-of-speech tags are defined in the [ACL2011 paper](http://www.ark.cs.cmu.edu/TweetNLP/gimpel+etal.acl11.pdf) \n",
      "and the [NAACL 2013 paper](http://www.ark.cs.cmu.edu/TweetNLP/owoputi+etal.naacl13.pdf), \n",
      "which also describe the data and gives some state-of-art results."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Data processing code\n",
      "\"\"\"\n",
      "def conllSeqGenerator(input_file,max_insts=1000000):\n",
      "    \"\"\" return an instance generator for a filename\n",
      "    \n",
      "    The generator yields lists of words and tags.  \n",
      "    \"\"\"\n",
      "    cur_words = []\n",
      "    cur_tags = []\n",
      "    num_insts = 0\n",
      "    with open(input_file) as instances:\n",
      "        for line in instances:\n",
      "            if len(line.rstrip()) == 0:\n",
      "                if len(cur_words) > 0:\n",
      "                    num_insts += 1\n",
      "                    yield cur_words,cur_tags\n",
      "                    cur_words = []\n",
      "                    cur_tags = []\n",
      "            else:\n",
      "                parts = line.rstrip().split()\n",
      "                cur_words.append(parts[0])\n",
      "                if len(parts)>1:\n",
      "                    cur_tags.append(parts[1])\n",
      "                else: cur_tags.append(unk)\n",
      "        if len(cur_words)>0: \n",
      "            num_insts += 1\n",
      "            yield cur_words,cur_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Define the file names\n",
      "trainfile = 'oct27.train'\n",
      "devfile = 'oct27.dev'\n",
      "testfile = 'oct27.test' # You do not have this for now\n",
      "offset = \"**OFFSET**\"\n",
      "unknown = \"**UNKNOWN**\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is a demo code for using function \"conllSeqGenerator()\""
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Demo\n",
      "alltags = set()\n",
      "for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)):    \n",
      "    for tag in tags:\n",
      "        alltags.add(tag)\n",
      "print alltags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "set(['!', '#', '$', '&', ',', 'A', '@', 'E', 'D', 'G', 'M', 'L', 'O', 'N', 'P', 'S', 'R', 'U', 'T', 'V', 'Y', 'X', 'Z', '^', '~'])\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1a** (1 point): Use the Counter class to identify the most common three words for each tag, in the training set. \n",
      "The most_common() function of the Counter class will help you here. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# your code here\n",
      "\n",
      "atags = Counter()\n",
      "for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)):    \n",
      "    for ix in xrange(len(tags)):\n",
      "        if tags[ix] not in atags:\n",
      "            atags[tags[ix]] = Counter([words[ix]])\n",
      "        else:\n",
      "            atags[tags[ix]] += Counter([words[ix]])    \n",
      "for tag in atags:\n",
      "        print tag, atags[tag].most_common(3)\n",
      "   \n",
      "            \n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "! [('lol', 54), ('Lol', 22), ('oh', 10)]\n",
        "# [('#nowplaying', 3), ('#np', 2), ('#tcot', 2)]\n",
        "$ [('one', 32), ('4', 6), ('2010', 6)]\n",
        "& [('and', 117), ('&', 39), ('but', 31)]\n",
        ", [('.', 427), ('!', 244), (',', 225)]\n",
        "A [('good', 24), ('new', 22), ('more', 13)]\n",
        "@ [('@Fresh32Prince89', 6), ('@lil_jeezy_85', 2), ('@ResourcefulMom', 2)]\n",
        "E [(':)', 28), ('<3', 10), (';)', 8)]\n",
        "D [('the', 236), ('a', 165), ('my', 89)]\n",
        "G [('smh', 9), ('|', 7), ('-', 7)]\n",
        "M [(\"momma's\", 1), ('#LebronShould', 1), (\"Ricochet's\", 1)]\n",
        "L [(\"I'm\", 42), ('its', 24), ('im', 15)]\n",
        "O [('I', 258), ('you', 135), ('it', 87)]\n",
        "N [('day', 19), ('time', 18), ('people', 17)]\n",
        "P [('to', 231), ('of', 112), ('for', 101)]\n",
        "S [('mans', 1), (\"judge's\", 1), ('year\\xe2\\x80\\x99s', 1)]\n",
        "R [('just', 56), ('not', 27), ('now', 26)]\n",
        "U [('http', 4), (':/', 1), ('http://tinyurl.com/2dt3o7n', 1)]\n",
        "T [('out', 29), ('up', 26), ('on', 8)]\n",
        "V [('is', 105), ('are', 52), ('was', 48)]\n",
        "Y [(\"there's\", 2)]\n",
        "X [('all', 6), ('There', 4), ('there', 2)]\n",
        "Z [(\"Obamacare's\", 1), ('\\xe2\\x80\\x9cLasers\\xe2\\x80\\x99s\\xe2\\x80\\x9d', 1), (\"Wayne's\", 1)]\n",
        "^ [('Heat', 8), ('Halloween', 5), ('twitter', 5)]\n",
        "~ [('RT', 229), (':', 207), ('...', 59)]\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Most-common-tag baseline ##\n",
      "\n",
      "Build a classifier that chooses the most common tag for each word.\n",
      "\n",
      "- You should implement your classifier in terms of a set of weights, as in pset 1\n",
      "- Prediction should use your predict() function from pset 1\n",
      "- For unseen words, the classifier should choose the tag with the most **unique** word types"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "argmax = lambda x : max(x.iteritems(),key=operator.itemgetter(1))[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# paste your predict function here\n",
      "def predict(instance,weights,labels):\n",
      "    tempscores = defaultdict(int)\n",
      "    for label in labels:\n",
      "                tempscores[label] = sum([weights[(label,feature)]*instance[feature] for feature in instance])\n",
      "                #tempscores[label] = sum([w_dict[feature]*new_inst_dict[feature] for feature in new_inst_dict])\n",
      "                    #append((label,sum([w_dict[feature]*new_inst_dict[feature] for feature in new_inst_dict])))\n",
      "    maxlabel = argmax(tempscores)\n",
      "    values = [maxlabel,zip(tempscores.keys(),tempscores.values())]\n",
      "    return values\n",
      "    return argmax(scores),scores"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "evalTagger evaluates a tagger. It takes three arguments:\n",
      "\n",
      "- a tagger, which is a **function** taking a list of words and a tagset as arguments\n",
      "- an output filename\n",
      "- a test file\n",
      "\n",
      "You will want to use lambda expressions to create the first argument for this function, as shown below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def evalTagger(tagger,outfilename,testfile=devfile):    \n",
      "    with open(outfilename,'w') as outfile:\n",
      "        for words,_ in conllSeqGenerator(testfile):\n",
      "            pred_tags = tagger(words,alltags)\n",
      "            for tag in pred_tags:\n",
      "                #print tag\n",
      "                print >>outfile, tag\n",
      "            print >>outfile, \"\"\n",
      "    return scorer.getConfusion(testfile,outfilename) #run the scorer on the prediction file"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here's how it works. I provide a tagger that labels everything as a noun."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# the list comprehension creates a list of 'N' equal to the length of the list of words\n",
      "noun_tagger = lambda words, alltags : ['N' for word in words]\n",
      "confusion = evalTagger(noun_tagger,'nouns')\n",
      "\n",
      "print scorer.accuracy(confusion)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.136844287788\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define a function called classifier tagger, with the following signature:\n",
      "- **input 1** a list of words\n",
      "- **input 2** a dict of weights\n",
      "- **input 3** a list of all possible tags\n",
      "- **output 1** a list of predicted tags\n",
      "\n",
      "Your function should call predict, using the weights."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define a function called classifierTagger\n",
      "def classifierTagger(words,weights,all_tags):\n",
      "        tags = []\n",
      "        for word in words:\n",
      "            c = Counter([word,offset])\n",
      "            tags.append(predict(c,weights,all_tags)[0])\n",
      "        return tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1b** (1 point) Use your classifierTagger to reproduce the \"always noun\" tagger above, this time by using the offset weight. \n",
      "\n",
      "**Sanity check** the accuracy should be the same as above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "your_weights = defaultdict(float)\n",
      "# your code: set the weights\n",
      "your_weights.update({('N',offset):1})\n",
      "noun_tagger_2 = lambda words, alltags: classifierTagger(words, your_weights, alltags)\n",
      "print scorer.accuracy(evalTagger(noun_tagger_2,'nouns'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.136844287788\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 1c** (2 points) Implement a tagger that selects the most common tag for each word. \n",
      "- You should use classifierTagger for this, so select the weights appropriately\n",
      "- To do this, you'll want to construct a bunch of counters, similar to what you did above.\n",
      "- For words that do not appear in the training data, select the tag which applies to the most word *types* in the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "awords = Counter()\n",
      "tags = []\n",
      "atags2 = Counter()\n",
      "weights = defaultdict(float)\n",
      "for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)):    \n",
      "    for ix in xrange(len(tags)):\n",
      "        if words[ix] not in awords:\n",
      "            awords[words[ix]] = Counter([tags[ix]])\n",
      "            #print Counter([atags[ix]])\n",
      "        else:\n",
      "            awords[words[ix]] += Counter([tags[ix]])  \n",
      "            #print awords[words[ix]]\n",
      "        if tags[ix] not in atags2:\n",
      "            atags2[tags[ix]] = Counter([words[ix]])\n",
      "        else:\n",
      "            if words[ix] not in atags2[tags[ix]]:\n",
      "                mcounter = atags2[tags[ix]]\n",
      "                mcounter[words[ix]] = 1\n",
      "                atags2[tags[ix]] = mcounter\n",
      "v = {}\n",
      "for tag in atags2:\n",
      "    v[tag] = sum([value for value in atags2[tag].values()])\n",
      "maxTag = argmax(v)\n",
      "\n",
      "\n",
      "for word in awords:\n",
      "    b = argmax(awords[word])\n",
      "    weights[(b,word)] = 2\n",
      "\n",
      "weights[(maxTag,offset)] = 1\n",
      "             \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check** my accuracy is approximately 70%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words,alltags : classifierTagger(words,weights,alltags),'mcc')\n",
      "print scorer.accuracy(confusion)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.702260004147\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2 Naive Bayes Classification #\n",
      "\n",
      "\n",
      "Write a function that builds a Naive Bayes classifier to perform supervised classification.\n",
      "\n",
      "- The base features should just be the word itself, plus an offset feature. \n",
      "- The output of this function should be weights that you can plug into the predict function that you wrote last time\n",
      "- An input should be a smoothing value $\\alpha$\n",
      "- **Hint**: You may want to use the counters that you built in problem 1a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def buildWordCounters():\n",
      "    word_counters = Counter()\n",
      "    tag_counters = Counter()\n",
      "    for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)): \n",
      "        word_counters += Counter(words)  \n",
      "        for ix in xrange(len(tags)):\n",
      "            if tags[ix] not in tag_counters:\n",
      "                tag_counters[tags[ix]] = Counter([words[ix]])\n",
      "            else:\n",
      "                tag_counters[tags[ix]] += Counter([words[ix]])\n",
      "    return word_counters,tag_counters"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_counters,tag_counters = buildWordCounters()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "l_count = defaultdict(int)\n",
      "for tag in alltags:\n",
      "    l_count[tag] = sum([count for count in tag_counters[tag].values()])\n",
      "\n",
      "  \n",
      "#totalWords = sum([value for value in word_counters.values()])\n",
      "totalWords = len(word_counters.keys())\n",
      "\n",
      "def getNBWeights(alpha):\n",
      "    weights = defaultdict(lambda : -1000.) #default value is a low log-probability\n",
      "    # your code here\n",
      "    for word in word_counters.keys():\n",
      "        for tag in alltags:\n",
      "            weights[(tag,offset)] = math.log((float(l_count[tag])/(sum(l_count.values()))))\n",
      "            if word != offset:\n",
      "                weights[(tag,word)] = tag_counters[tag][word] + alpha\n",
      "                #weights[(tag,word)] = atags2[tag][word]\n",
      "                #weights[(tag,word)] = math.log((weights[(tag,word)])/(float(l_count[tag]))) \n",
      "                weights[(tag,word)] = math.log((weights[(tag,word)])/(float(l_count[tag])+totalWords*alpha))\n",
      "    return weights"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: your code should give the same results I get below"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w1 = getNBWeights(.1)\n",
      "# print total probability mass for a tag\n",
      "\n",
      "print sum([np.exp(w1[('N',word)]) for word in word_counters.keys()])\n",
      "# print some common values\n",
      "print w1[('N','breakfast')], w1[('V','breakfast')], w1[('A','smart')], w1[('D','the')], w1[('!',offset)]\n",
      "#5434.00023146"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n",
        "-7.74708642426 -10.226404038 -6.42687365064 -1.78871941598 -3.58372417191\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dev_acc = dict()\n",
      "for alpha in [1e-4,1e-3,1e-2,1e-1,1e0,1e1]:\n",
      "    nb_weights = getNBWeights(alpha)\n",
      "    confusion = evalTagger(lambda words, alltags : classifierTagger(words,nb_weights,alltags),'nb')\n",
      "    dev_acc[alpha] = scorer.accuracy(confusion)\n",
      "    print alpha,dev_acc[alpha]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0001 0.67095168982\n",
        "0.001"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.67095168982\n",
        "0.01"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.67095168982\n",
        "0.1"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.66846361186\n",
        "1.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.625544267054\n",
        "10.0"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.510470661414\n"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 2** (3 points): run the code below to evaluate your naive bayes tagger on the development set.\n",
      "\n",
      "**Sanity check**: you may do a little worse than the most-common tag classifier"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 3 Viterbi Algorithm #\n",
      "\n",
      "In this section you will implement the Viterbi algorithm. As a reminder, here's the math:\n",
      "\n",
      "$\\renewcommand{\\vec}[1]{\\mathbf{#1}}$\n",
      "\\begin{align*}\n",
      "\\vec{w}^{\\top}\\vec{f}(\\vec{x},\\vec{y}) = & \\sum_i \\vec{w}^{\\top} \\vec{f}(\\vec{x},y_i,y_{i-1},i) \\\\\n",
      "v(y,0) = & \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,\\diamond,0)\\\\\n",
      "b(y,0) = & \\diamond \\\\\n",
      "v(y,i) = & \\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
      "b(y,i-1) = & \\text{arg}\\max_{y'} \\vec{w}^{\\top}\\vec{f}(\\vec{x},y,y',i) + v(y',i-1)\\\\\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To get warmed up, let's work out an example by hand. There are only two tags, \n",
      "N and V. Here are the parameters:\n",
      "\n",
      "| | Value |\n",
      "| ------------- |:-------------:|\n",
      "| $\\log P_E(\\cdot|N)$ | they: -1, can: -3, fish: -3 |\n",
      "| $\\log P_E(\\cdot|V)$ | they: -10, can: -2, fish: -3 |\n",
      "| $\\log P_T(\\cdot|N)$ | N: -5, V: -2, END: -2 |\n",
      "| $\\log P_T(\\cdot|V)$ | N: -1, V: -4, END: -3 |\n",
      "| $\\log P_T(\\cdot|\\text{START})$ | N :-1, V :-1 |\n",
      "\n",
      "where $P_E(\\cdot|\\cdot)$ is the emission probability and $P_T(\\cdot|\\cdot)$ is the translation probability.\n",
      " \n",
      "In class we discuss the sentence *They can fish*. Now work out a more complicated example: \"*They can can fish*\".\n",
      " \n",
      "** Deliverable 3a ** (1 point) Show the trellis-like table, and give the score for the best best scoring path(s). After you work out the trellis by hand, you should be able to fill the following table.\n",
      "\n",
      "\n",
      "** Sanity check ** There are two paths that each score -18."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*(Fill your answer in the following table)*\n",
      "\n",
      "|POS tag| START  | they | can | can | fish | END |\n",
      "|-------|:-------|:-----|:----|:----|:-----|:---:|\n",
      "| N     |    0   | -2   | -10 | -10 | -16  | -18 |\n",
      "| V     |    0   | -11  |   -6| -12 | -15  | -18 |"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Implementing Viterbi ##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# you can see how these are used in the example weights below\n",
      "start_tag = '--START--'\n",
      "end_tag = '--END--'\n",
      "trans ='--TRANS--'\n",
      "emit = '--EMISSION--'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following function computes the HMM features for the function $\\vec{f}(\\vec{x},y,y',i)$. \n",
      "- You will call it in your viterbi tagger. \n",
      "- Note that it returns both an emission and transition feature, except for the last word, where it returns only a transition feature. \n",
      "- Also note that transition and emission features are specially marked"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def hmm_feats(words,curr_tag,prev_tag,i):\n",
      "    if i < len(words):\n",
      "        return [(curr_tag,words[i],emit),(curr_tag,prev_tag,trans)]\n",
      "    else: return [(curr_tag,prev_tag,trans)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some predefined weights, corresponding to problem 3a."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "defined_weights = {('N','they',emit):-1,('N','can',emit):-3,('N','fish',emit):-3,\\\n",
      "                        ('V','they',emit):-10,('V','can',emit):-2,('V','fish',emit):-3,\\\n",
      "                        ('N','N',trans):-5,('V','N',trans):-2,(end_tag,'N',trans):-2,\\\n",
      "                        ('N','V',trans):-1,('V','V',trans):-4,(end_tag,'V',trans):-3,\\\n",
      "                        ('N',start_tag,trans):-1,('V',start_tag,trans):-1}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Define viterbiTagger\n",
      "\n",
      "- **Input 1**: a list of words\n",
      "- **Input 2**: a feature function, like hmm_feats\n",
      "- **Input 3**: a dict of weights\n",
      "- **Input 4**: a list of all possible tags\n",
      "- **Output 1**: the best-scoring sequence\n",
      "- **Output 2**: the score of the best-scoring sequence\n",
      "\n",
      "Hint: you can represent the trellis and the back pointers as lists of dicts. You will want to do some special handling for the first and last words; otherwise, just iterate "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "def viterbiTagger(words,feat_func,weights,all_tags,debug=False):\n",
      "    trellis = [defaultdict(float) for k in xrange(len(words))]\n",
      "    #hint: store the $v$ table here \n",
      "    #trellis = np.matlib.zeros((len(all_tags), len(words)))\n",
      "    pointers = [defaultdict(str) for k in xrange(len(words))]#hint: store the $b$ table here\n",
      "    output = [None] * len(words) #hint: store the output here. build this last.\n",
      "    best_score = 0\n",
      "    #start state\n",
      "    trellis[0] = {tag:0 for tag in all_tags}\n",
      "    #backpointers to start state\n",
      "    pointers[0] = {tag:start_tag for tag in all_tags}\n",
      "    # your code here\n",
      "    for w_i, word in enumerate(words):\n",
      "        for tag in all_tags:\n",
      "            max_nu = float('-inf')\n",
      "            if w_i == 0:\n",
      "                p_tup = feat_func(words,tag,start_tag,w_i)\n",
      "                #print p_tup\n",
      "                curr = weights[p_tup[0]]+weights[p_tup[1]]\n",
      "                trellis[w_i][tag] = curr\n",
      "               \n",
      "            else:\n",
      "                for prev_tag in all_tags:\n",
      "                    p_tup = feat_func(words,tag,prev_tag,w_i)\n",
      "                    #print trellis[w_i]\n",
      "                    #p_tup[0] = emit prob, p_tup[1] = trans_prob\n",
      "                    \n",
      "                    curr = weights[p_tup[0]]+weights[p_tup[1]]+ trellis[w_i-1][prev_tag] \n",
      "                    \n",
      "                    if curr > max_nu:\n",
      "                        max_nu = curr\n",
      "                        \n",
      "                        #print w_i,curr, trellis[w_i-1][prev_tag],tag,prev_tag\n",
      "                        prev_p = prev_tag\n",
      "                #print word,w_i, max_nu, prev_p \n",
      "                trellis[w_i][tag] = max_nu\n",
      "                \n",
      "                pointers[w_i][tag] = prev_p\n",
      "            \n",
      "               # print 'done updating', w_i, tag, pointers[w_i]\n",
      "               # print w_i, pointers[w_i], 'word index = '\n",
      "                #print '1b', pointers[1]\n",
      "               \n",
      "    w_i = len(words)\n",
      "    max_nu = float('-inf')\n",
      "    prev_max_t = None\n",
      "    for prev_tag in all_tags:\n",
      "        #list of a tuple\n",
      "        p_tup_end = feat_func(words,end_tag,prev_tag, w_i)\n",
      "        curr = weights[p_tup_end[0]]+ trellis[w_i-1][prev_tag]\n",
      "        #print w_i-1, trellis[w_i-1][prev_tag], prev_tag\n",
      "        #print w_i-1, weights[p_tup_end[0]],prev_tag,p_tup_end[0]\n",
      "        if curr > max_nu:\n",
      "            max_nu = curr\n",
      "            prev_max_t = prev_tag\n",
      "       \n",
      "    prev_p = prev_max_t\n",
      "   \n",
      "    best_score = max_nu\n",
      "    \n",
      "    ctag = prev_p;\n",
      "    \n",
      "    for ix,w in reversed(list(enumerate(words))):\n",
      "         #print pointers[ix]\n",
      "         output[ix] = (ctag)\n",
      "         ctag = pointers[ix][ctag]\n",
      "         \n",
      "\n",
      "    return output,best_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3b** (1 point) run you viterbi tagger on the example in 3a, using the code below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# keep this code\n",
      "viterbiTagger(['they','can','can', 'fish'],hmm_feats,defined_weights,['N','V'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "(['N', 'V', 'V', 'N'], -18)"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3c** (1 point) run your Viterbi on the following example"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = 'they can can can can can can can fish'.split()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "viterbiTagger(sent,hmm_feats,defined_weights,['N','V'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "(['N', 'V', 'N', 'V', 'N', 'V', 'N', 'V', 'N'], -36)"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now estimate the weights of a hidden Markov model. \n",
      "- You have already estimated the emission weights $\\log P(w | y)$, in your solution to problem 2. Use your solution with $\\alpha=0.001$\n",
      "- Estimate the transition probabilities from the training data, using the maximum-likelihood estimates (no smoothing)\n",
      "- Don't forget transitions from the start state and to the end state"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nb_weights = getNBWeights(0.001) #compute naive bayes weights\n",
      "# convert nb weights to hmm weights\n",
      "hmm_weights = defaultdict(lambda : -1000.)\n",
      "for tag,word in nb_weights:\n",
      "    hmm_weights[(tag,word,emit)] = nb_weights[(tag,word)]\n",
      "\n",
      "print l_count"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "defaultdict(<type 'int'>, {'!': 406, '#': 141, '$': 216, '&': 239, ',': 1715, 'A': 755, '@': 713, 'E': 148, 'D': 869, 'G': 137, 'M': 3, 'L': 252, 'O': 1063, 'N': 2003, 'P': 1252, 'S': 18, 'R': 689, 'U': 223, 'T': 92, 'V': 2219, 'Y': 2, 'X': 15, 'Z': 21, '^': 890, '~': 538})\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute tag-to-tag transition counts, add them to hmm_weights      \n",
      "def hmm_tagfeats(words,curr_tag,prev_tag,i):\n",
      "    if i < len(words):\n",
      "        return[(curr_tag,prev_tag,trans)]\n",
      "    else: return [(curr_tag,prev_tag,trans)]\n",
      "c = 0\n",
      "length = 0\n",
      "\n",
      "tag_trans_counts = defaultdict(lambda : Counter())\n",
      "for i,(words, tags) in enumerate(conllSeqGenerator(trainfile)): \n",
      "    length+= len(tags)\n",
      "    prev_tag = start_tag;\n",
      "    ix = 0\n",
      "    for tag in tags:\n",
      "        feats = hmm_tagfeats(words, tag, prev_tag,ix)\n",
      "        t = feats[0]\n",
      "        tag_trans_counts[feats[0]] += Counter(feats)\n",
      "        prev_tag=tag\n",
      "        ix +=1\n",
      "    feats = hmm_tagfeats(words,end_tag,prev_tag,ix)\n",
      "    tag_trans_counts[feats[0]] += Counter(feats)\n",
      "\n",
      "start_class = i \n",
      "\n",
      "for tup in tag_trans_counts:\n",
      "    if tup[1] == start_tag:\n",
      "         hmm_weights[tup] = math.log(tag_trans_counts[tup][tup]/float(start_class))\n",
      "    else:\n",
      "        hmm_weights[tup] = math.log(tag_trans_counts[tup][tup]/float(l_count[tup[1]]))\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: here are some weights that I estimate. Yours should be very close, if not identical."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print hmm_weights['V','go',emit], hmm_weights['~','go',emit], hmm_weights['^','diddy',emit]\n",
      "print hmm_weights['V','V',trans], hmm_weights['~','V',trans]\n",
      "print hmm_weights[end_tag,'V',trans], hmm_weights[end_tag,'~',trans]\n",
      "\"\"\"\n",
      "-4.66268727503 -13.2056617029 -1000.0\n",
      "-1.89367092996 -5.9130524537\n",
      "-4.30361454127 -3.06898273529\n",
      "\"\"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-4.66268727503 -13.2056617029 -1000.0\n",
        "-1.89367092996 -5.9130524537\n",
        "-4.30361454127 -3.06898273529\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "'\\n-4.66268727503 -13.2056617029 -1000.0\\n-1.89367092996 -5.9130524537\\n-4.30361454127 -3.06898273529\\n'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Sanity check**: here's the tag sequence and score for a modified version of our example sentence"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "viterbiTagger([':))','we','can','can','fish',':-)'],hmm_feats,hmm_weights,alltags)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "(['E', 'O', 'V', 'V', 'N', 'E'], -47.969198217657684)"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Deliverable 3d** (3 points): compute the predicted tag sequence and scores for the first three sentence in the training data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i,(words,_) in enumerate(conllSeqGenerator(trainfile)):\n",
      "    print i, viterbiTagger(words,hmm_feats,hmm_weights,alltags)\n",
      "    if i >= 2: break"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0 (['O', 'V', 'O', 'V', 'V', 'D', 'A', 'N', 'O', 'V', 'T', ',', 'V', '^', '^', 'N', ',', 'R', 'P', 'O', 'V', 'L', 'P', 'O', '~', '@', '~', '^', ',', '~', ',', 'U'], -204.41594281646036)\n",
        "1 "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(['~', '@', '~', 'O', 'N', 'V', 'P', 'D', 'N', 'N', ','], -77.94102394393093)\n",
        "2 (['^', 'A', '^', '$', ',', 'V', 'D', 'A', 'N', 'E'], -71.35227969447283)\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "** Deliverable 3d** (2 points):\n",
      "- Run your HMM tagger on the dev data, using the code line below.\n",
      "- ** Sanity check**: I get 74.5% accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "confusion = evalTagger(lambda words, alltags : viterbiTagger(words,hmm_feats,hmm_weights,alltags)[0],'hmm')\n",
      "\n",
      "print scorer.accuracy(confusion)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.745386688783\n"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}